# 图书APP用户模块 - 架构优化深度分析文档

## 文档信息
- **文档版本**: 1.0
- **创建日期**: 2025-10-14
- **思考模式**: UltraThink深度分析
- **适用场景**: 图书阅读APP用户系统架构设计与优化

---

## 1. 当前架构分析

### 1.1 现有设计的优势

✅ **表结构清晰，职责分离**
- 基础信息表（users）与扩展信息表（user_profiles）分离
- 阅读数据（reading_records）与统计数据（reading_statistics）分离
- 认证数据与业务数据解耦

✅ **数据可追溯性**
- 采用软删除（deleted_at字段）
- 保留完整的登录日志（user_login_logs）
- 积分流水记录所有变动

✅ **数据一致性保障**
- 外键约束保证关联数据完整性
- 唯一索引防止重复数据
- 事务保证关键操作的原子性

✅ **灵活的扩展性**
- JSON字段存储动态配置（preferences, metadata）
- 预留扩展字段
- 支持多租户架构扩展

---

## 2. 性能瓶颈识别

### 2.1 高频写入问题

**问题描述：**
阅读进度更新是高频操作，前端每30秒上报一次阅读状态。

**数据量估算：**
```
假设：
- 同时在线用户：100,000人
- 上报频率：30秒/次
- 每分钟写入次数：100,000 × 2 = 200,000次
- 每小时写入次数：12,000,000次
- 每天写入次数：288,000,000次（2.88亿次）
```

**当前设计的问题：**
```sql
-- 每次上报都直接写入MySQL
UPDATE user_reading_records
SET current_page = ?,
    reading_progress = ?,
    reading_duration = reading_duration + ?,
    last_reading_at = NOW()
WHERE user_id = ? AND book_id = ?;
```

**导致的后果：**
1. ❌ MySQL写入压力巨大
2. ❌ 主从复制延迟增加
3. ❌ 锁竞争严重
4. ❌ 磁盘IO成为瓶颈

---

### 2.2 优化方案：Redis缓存 + 异步批量写入

#### 方案架构图

```
┌─────────────┐
│   客户端    │
│  (每30秒)   │
└──────┬──────┘
       │ 上报阅读进度
       ↓
┌─────────────────────────────────────┐
│           API Gateway               │
└──────┬──────────────────────────────┘
       │
       ↓
┌─────────────────────────────────────┐
│       Reading Service               │
│  1. 验证数据合法性                  │
│  2. 写入Redis（实时）               │
│  3. 发送到消息队列（异步）          │
└──────┬──────────────────────────────┘
       │
       ├─────────────────┬──────────────┐
       ↓                 ↓              ↓
┌─────────────┐   ┌─────────────┐  ┌──────────────┐
│   Redis     │   │ Kafka/MQ    │  │   MySQL      │
│  (热数据)   │   │  (消息队列) │  │ (持久化存储) │
│             │   │             │  │              │
│ TTL: 1小时  │   └──────┬──────┘  │              │
└─────────────┘          │         │              │
                         ↓         │              │
                  ┌──────────────┐ │              │
                  │Batch Writer  │ │              │
                  │ (每5分钟)    ├─┘              │
                  │ 批量写入MySQL│                │
                  └──────────────┘                │
                                                  │
                  ┌──────────────────────────────┘
                  │
                  ↓
           ┌──────────────┐
           │ Slave MySQL  │
           │  (读取查询)  │
           └──────────────┘
```

#### Redis数据结构设计

```redis
# 1. 阅读进度（Hash结构）
KEY: reading:progress:{user_id}:{book_id}
HSET reading:progress:1001:5001 current_page 156
HSET reading:progress:1001:5001 progress 45.5
HSET reading:progress:1001:5001 chapter_id 8
HSET reading:progress:1001:5001 duration 1800
HSET reading:progress:1001:5001 last_update 1728907200000
EXPIRE reading:progress:1001:5001 3600  # 1小时过期

# 2. 待同步队列（Sorted Set，按时间排序）
KEY: reading:sync_queue
ZADD reading:sync_queue 1728907200 "1001:5001"

# 3. 用户当前阅读会话（String）
KEY: reading:session:{user_id}
SET reading:session:1001 '{"book_id":5001,"start_time":1728907200,"device_id":"device_123"}'
EXPIRE reading:session:1001 7200  # 2小时过期

# 4. 阅读时长统计（Hash，按日期）
KEY: reading:daily_stats:{user_id}:{date}
HINCRBY reading:daily_stats:1001:20251014 total_minutes 30
HINCRBY reading:daily_stats:1001:20251014 session_count 1
EXPIRE reading:daily_stats:1001:20251014 86400  # 24小时后同步到MySQL
```

#### 批量写入逻辑（伪代码）

```python
class ReadingProgressBatchWriter:
    """阅读进度批量写入器"""

    def __init__(self):
        self.redis_client = RedisClient()
        self.mysql_client = MySQLClient()
        self.batch_size = 1000
        self.interval = 300  # 5分钟

    async def run(self):
        """定时任务：每5分钟执行一次"""
        while True:
            await asyncio.sleep(self.interval)
            await self.batch_sync_to_mysql()

    async def batch_sync_to_mysql(self):
        """批量同步到MySQL"""
        # 1. 从Redis获取待同步的数据
        current_time = int(time.time())
        # 获取5分钟前到现在的所有记录
        records = self.redis_client.zrangebyscore(
            'reading:sync_queue',
            current_time - 300,
            current_time
        )

        if not records:
            return

        # 2. 批量获取详细数据
        pipeline = self.redis_client.pipeline()
        for record in records:
            user_id, book_id = record.split(':')
            pipeline.hgetall(f'reading:progress:{user_id}:{book_id}')

        progress_data_list = pipeline.execute()

        # 3. 构建批量更新SQL
        update_values = []
        for i, record in enumerate(records):
            user_id, book_id = record.split(':')
            data = progress_data_list[i]

            if not data:
                continue

            update_values.append({
                'user_id': user_id,
                'book_id': book_id,
                'current_page': data.get('current_page', 0),
                'reading_progress': data.get('progress', 0),
                'reading_duration': data.get('duration', 0),
                'last_reading_at': datetime.fromtimestamp(
                    int(data.get('last_update', 0)) / 1000
                )
            })

        # 4. 批量执行 INSERT ... ON DUPLICATE KEY UPDATE
        if update_values:
            await self.batch_upsert(update_values)

            # 5. 从同步队列中移除已处理的记录
            self.redis_client.zremrangebyscore(
                'reading:sync_queue',
                current_time - 300,
                current_time
            )

    async def batch_upsert(self, values):
        """批量插入或更新"""
        sql = """
        INSERT INTO user_reading_records
        (user_id, book_id, current_page, reading_progress,
         reading_duration, last_reading_at, updated_at)
        VALUES (%s, %s, %s, %s, %s, %s, NOW())
        ON DUPLICATE KEY UPDATE
            current_page = VALUES(current_page),
            reading_progress = VALUES(reading_progress),
            reading_duration = reading_duration + VALUES(reading_duration),
            last_reading_at = VALUES(last_reading_at),
            updated_at = NOW()
        """

        # 分批执行，避免单次事务过大
        for i in range(0, len(values), self.batch_size):
            batch = values[i:i + self.batch_size]
            batch_params = [
                (v['user_id'], v['book_id'], v['current_page'],
                 v['reading_progress'], v['reading_duration'],
                 v['last_reading_at'])
                for v in batch
            ]

            await self.mysql_client.executemany(sql, batch_params)
```

#### 关键场景处理

**场景1：用户关闭APP**
```python
async def on_app_close(user_id: int, book_id: int):
    """用户关闭APP时，立即同步数据"""
    # 1. 从Redis获取最新进度
    progress_data = redis_client.hgetall(
        f'reading:progress:{user_id}:{book_id}'
    )

    # 2. 立即写入MySQL（不等待批量任务）
    await mysql_client.execute("""
        INSERT INTO user_reading_records (...)
        VALUES (...)
        ON DUPLICATE KEY UPDATE ...
    """)

    # 3. 清理Redis缓存（可选，也可以等待过期）
    redis_client.delete(f'reading:progress:{user_id}:{book_id}')
    redis_client.zrem('reading:sync_queue', f'{user_id}:{book_id}')
```

**场景2：查询用户阅读进度**
```python
async def get_reading_progress(user_id: int, book_id: int):
    """获取阅读进度（优先从Redis读取）"""
    # 1. 先从Redis读取（热数据）
    progress = redis_client.hgetall(
        f'reading:progress:{user_id}:{book_id}'
    )

    if progress:
        return {
            'current_page': int(progress['current_page']),
            'progress': float(progress['progress']),
            'reading_duration': int(progress['duration']),
            'last_reading_at': datetime.fromtimestamp(
                int(progress['last_update']) / 1000
            )
        }

    # 2. Redis没有，从MySQL读取（冷数据）
    record = await mysql_client.fetchone("""
        SELECT current_page, reading_progress, reading_duration, last_reading_at
        FROM user_reading_records
        WHERE user_id = %s AND book_id = %s
    """, (user_id, book_id))

    if record:
        # 3. 回写到Redis（预热缓存）
        redis_client.hmset(
            f'reading:progress:{user_id}:{book_id}',
            {
                'current_page': record['current_page'],
                'progress': record['reading_progress'],
                'duration': record['reading_duration'],
                'last_update': int(record['last_reading_at'].timestamp() * 1000)
            }
        )
        redis_client.expire(f'reading:progress:{user_id}:{book_id}', 3600)

        return record

    return None
```

**场景3：服务器重启或Redis故障**
```python
async def recover_from_redis_failure():
    """Redis故障恢复处理"""
    # 1. 降级方案：直接写MySQL（牺牲性能保证数据不丢失）
    if not redis_client.ping():
        logger.warning("Redis is down, fallback to MySQL direct write")
        return await mysql_client.execute(...)

    # 2. Redis恢复后，从MySQL预热热点数据
    async def preheat_hot_data():
        # 获取最近1小时活跃的用户阅读记录
        hot_records = await mysql_client.fetchall("""
            SELECT user_id, book_id, current_page, reading_progress,
                   reading_duration, UNIX_TIMESTAMP(last_reading_at) * 1000 as last_update
            FROM user_reading_records
            WHERE last_reading_at >= NOW() - INTERVAL 1 HOUR
        """)

        # 批量写入Redis
        pipeline = redis_client.pipeline()
        for record in hot_records:
            key = f"reading:progress:{record['user_id']}:{record['book_id']}"
            pipeline.hmset(key, {
                'current_page': record['current_page'],
                'progress': record['reading_progress'],
                'duration': record['reading_duration'],
                'last_update': record['last_update']
            })
            pipeline.expire(key, 3600)

        pipeline.execute()
```

---

### 2.3 性能对比

#### 优化前（直接写MySQL）
```
QPS限制：约 5,000 - 10,000 写操作/秒
响应时间：50-200ms（高峰期）
数据库连接数：200-500个
主从延迟：5-30秒
```

#### 优化后（Redis + 批量写入）
```
QPS限制：100,000+ 写操作/秒（Redis）
响应时间：5-15ms
数据库连接数：50-100个
主从延迟：1-5秒
MySQL写入次数：减少95%（从每秒10,000次降到500次）
```

#### 成本节省
```
数据库实例规格：从 16核64GB 降到 8核32GB
成本节省：约 40-50%
缓存成本：Redis 8GB集群（约20%原数据库成本）
总体成本：节省 30-40%
```

---

## 3. 统计数据的实时性问题

### 3.1 问题分析

**现有设计：**
```sql
CREATE TABLE user_reading_statistics (
    user_id BIGINT NOT NULL,
    stat_date DATE NOT NULL,
    stat_type ENUM('daily', 'weekly', 'monthly', 'yearly'),
    books_read INT DEFAULT 0,
    reading_minutes INT DEFAULT 0,
    ...
);
```

**问题：**
1. ❌ 用户查看统计时需要实时计算，查询慢
2. ❌ 聚合查询（GROUP BY）在大数据量下性能差
3. ❌ 无法快速响应"实时统计"需求（如：今天已阅读30分钟）

---

### 3.2 优化方案：预计算 + 增量更新

#### 架构设计

```
用户行为触发
      ↓
┌─────────────────┐
│  业务服务层     │
│  (读书/笔记等)  │
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│  发送事件到MQ   │ ← 异步解耦
└────────┬────────┘
         │
         ↓
┌─────────────────────────────┐
│  Statistics Consumer        │
│  1. 消费事件                │
│  2. 增量更新Redis统计       │
│  3. 定时落库（每小时）      │
└────────┬────────────────────┘
         │
         ├──────────────┬───────────────┐
         ↓              ↓               ↓
   ┌─────────┐   ┌──────────┐   ┌──────────┐
   │  Redis  │   │  MySQL   │   │  OLAP    │
   │ (实时)  │   │ (持久化) │   │ (分析)   │
   └─────────┘   └──────────┘   └──────────┘
```

#### Redis统计数据结构

```redis
# 1. 用户今日统计（Hash）
KEY: stats:daily:{user_id}:{date}
HSET stats:daily:1001:20251014 books_read 2
HSET stats:daily:1001:20251014 reading_minutes 120
HSET stats:daily:1001:20251014 pages_read 89
HSET stats:daily:1001:20251014 notes_created 5
EXPIRE stats:daily:1001:20251014 172800  # 2天后过期

# 2. 用户本周统计（Hash）
KEY: stats:weekly:{user_id}:{year}-W{week}
HSET stats:weekly:1001:2025-W42 books_read 5
HSET stats:weekly:1001:2025-W42 reading_minutes 450

# 3. 用户本月统计（Hash）
KEY: stats:monthly:{user_id}:{year}-{month}
HSET stats:monthly:1001:2025-10 books_read 12
HSET stats:monthly:1001:2025-10 reading_minutes 1800

# 4. 用户年度统计（Hash）
KEY: stats:yearly:{user_id}:{year}
HSET stats:yearly:1001:2025 books_read 85
HSET stats:yearly:1001:2025 reading_minutes 15000

# 5. 连续阅读天数（Sorted Set，存储阅读日期）
KEY: stats:streak:{user_id}
ZADD stats:streak:1001 20251014 20251014
ZADD stats:streak:1001 20251013 20251013
ZADD stats:streak:1001 20251012 20251012
```

#### 增量更新实现

```python
class StatisticsUpdater:
    """统计数据增量更新器"""

    def __init__(self):
        self.redis = RedisClient()
        self.mysql = MySQLClient()

    async def on_reading_session_end(self, event):
        """阅读会话结束时更新统计"""
        user_id = event['user_id']
        book_id = event['book_id']
        duration_minutes = event['duration_minutes']
        pages_read = event['pages_read']
        date = event['date']  # 格式：20251014

        # 1. 更新今日统计
        pipeline = self.redis.pipeline()

        daily_key = f'stats:daily:{user_id}:{date}'
        pipeline.hincrby(daily_key, 'reading_minutes', duration_minutes)
        pipeline.hincrby(daily_key, 'pages_read', pages_read)
        pipeline.hincrby(daily_key, 'reading_sessions', 1)
        pipeline.expire(daily_key, 172800)

        # 2. 更新本周统计
        year_week = datetime.strptime(date, '%Y%m%d').strftime('%Y-W%U')
        weekly_key = f'stats:weekly:{user_id}:{year_week}'
        pipeline.hincrby(weekly_key, 'reading_minutes', duration_minutes)
        pipeline.hincrby(weekly_key, 'pages_read', pages_read)
        pipeline.expire(weekly_key, 604800)  # 7天

        # 3. 更新本月统计
        year_month = date[:6]  # 202510
        monthly_key = f'stats:monthly:{user_id}:{year_month}'
        pipeline.hincrby(monthly_key, 'reading_minutes', duration_minutes)
        pipeline.hincrby(monthly_key, 'pages_read', pages_read)
        pipeline.expire(monthly_key, 2592000)  # 30天

        # 4. 更新年度统计
        year = date[:4]
        yearly_key = f'stats:yearly:{user_id}:{year}'
        pipeline.hincrby(yearly_key, 'reading_minutes', duration_minutes)
        pipeline.hincrby(yearly_key, 'pages_read', pages_read)
        pipeline.expire(yearly_key, 31536000)  # 365天

        # 5. 更新连续阅读天数
        streak_key = f'stats:streak:{user_id}'
        pipeline.zadd(streak_key, {date: int(date)})

        await pipeline.execute()

    async def on_book_finished(self, event):
        """完成一本书时更新统计"""
        user_id = event['user_id']
        date = event['date']

        pipeline = self.redis.pipeline()

        daily_key = f'stats:daily:{user_id}:{date}'
        pipeline.hincrby(daily_key, 'books_read', 1)

        year_week = datetime.strptime(date, '%Y%m%d').strftime('%Y-W%U')
        weekly_key = f'stats:weekly:{user_id}:{year_week}'
        pipeline.hincrby(weekly_key, 'books_read', 1)

        year_month = date[:6]
        monthly_key = f'stats:monthly:{user_id}:{year_month}'
        pipeline.hincrby(monthly_key, 'books_read', 1)

        year = date[:4]
        yearly_key = f'stats:yearly:{user_id}:{year}'
        pipeline.hincrby(yearly_key, 'books_read', 1)

        await pipeline.execute()

    async def get_user_statistics(self, user_id: int, stat_type: str):
        """获取用户统计数据（优先从Redis）"""
        today = datetime.now().strftime('%Y%m%d')

        if stat_type == 'daily':
            key = f'stats:daily:{user_id}:{today}'
        elif stat_type == 'weekly':
            year_week = datetime.now().strftime('%Y-W%U')
            key = f'stats:weekly:{user_id}:{year_week}'
        elif stat_type == 'monthly':
            year_month = today[:6]
            key = f'stats:monthly:{user_id}:{year_month}'
        elif stat_type == 'yearly':
            year = today[:4]
            key = f'stats:yearly:{user_id}:{year}'

        # 从Redis获取
        stats = self.redis.hgetall(key)

        if stats:
            return {
                'books_read': int(stats.get('books_read', 0)),
                'reading_minutes': int(stats.get('reading_minutes', 0)),
                'pages_read': int(stats.get('pages_read', 0)),
                'notes_created': int(stats.get('notes_created', 0))
            }

        # Redis没有，从MySQL读取并回填
        return await self.load_from_mysql_and_cache(user_id, stat_type, key)

    async def calculate_reading_streak(self, user_id: int):
        """计算连续阅读天数"""
        streak_key = f'stats:streak:{user_id}'

        # 获取最近100天的阅读记录（降序）
        dates = self.redis.zrevrange(streak_key, 0, 99)

        if not dates:
            return 0

        # 计算连续天数
        today = int(datetime.now().strftime('%Y%m%d'))
        streak = 0
        expected_date = today

        for date_str in dates:
            date_int = int(date_str)

            if date_int == expected_date:
                streak += 1
                expected_date -= 1  # 前一天（简化处理，实际需处理月份边界）
            elif date_int < expected_date:
                break

        return streak
```

#### 定时任务：落库

```python
class StatisticsPersister:
    """统计数据持久化器（定时任务）"""

    async def run_hourly(self):
        """每小时执行一次，将Redis数据同步到MySQL"""
        # 1. 获取所有今日统计的Key
        pattern = 'stats:daily:*:' + datetime.now().strftime('%Y%m%d')
        keys = self.redis.keys(pattern)

        # 2. 批量读取并写入MySQL
        for key in keys:
            _, _, user_id, date = key.split(':')
            stats = self.redis.hgetall(key)

            await self.mysql.execute("""
                INSERT INTO user_reading_statistics
                (user_id, stat_date, stat_type, books_read, reading_minutes,
                 pages_read, reading_sessions, notes_created, created_at)
                VALUES (%s, %s, 'daily', %s, %s, %s, %s, %s, NOW())
                ON DUPLICATE KEY UPDATE
                    books_read = VALUES(books_read),
                    reading_minutes = VALUES(reading_minutes),
                    pages_read = VALUES(pages_read),
                    reading_sessions = VALUES(reading_sessions),
                    notes_created = VALUES(notes_created),
                    updated_at = NOW()
            """, (
                user_id,
                datetime.strptime(date, '%Y%m%d').date(),
                stats.get('books_read', 0),
                stats.get('reading_minutes', 0),
                stats.get('pages_read', 0),
                stats.get('reading_sessions', 0),
                stats.get('notes_created', 0)
            ))
```

---

## 4. 大表优化：笔记表分表策略

### 4.1 问题分析

**活跃用户的笔记数量：**
```
假设：
- 活跃用户平均每本书创建20条笔记
- 活跃用户年均阅读50本书
- 单个活跃用户年度笔记数：20 × 50 = 1,000条
- 3年累计：3,000条笔记

如果有100万活跃用户：
- 总笔记数：3,000 × 1,000,000 = 30亿条记录
```

**单表30亿记录的问题：**
1. ❌ 查询缓慢（即使有索引）
2. ❌ 表空间巨大（100GB+）
3. ❌ 备份恢复困难
4. ❌ DDL操作（如增加字段）耗时长

---

### 4.2 分表方案设计

#### 方案1：按年份分表（推荐）

```sql
-- 2025年的笔记
CREATE TABLE user_reading_notes_2025 (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    book_id BIGINT NOT NULL,
    -- ... 其他字段
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_user_book (user_id, book_id),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB COMMENT='用户阅读笔记表-2025年';

-- 2024年的笔记
CREATE TABLE user_reading_notes_2024 (
    -- 结构同上
) ENGINE=InnoDB COMMENT='用户阅读笔记表-2024年';

-- 2023年的笔记
CREATE TABLE user_reading_notes_2023 (
    -- 结构同上
) ENGINE=InnoDB COMMENT='用户阅读笔记表-2023年';
```

#### 路由中间件实现

```python
class NotesTableRouter:
    """笔记表路由器"""

    @staticmethod
    def get_table_name(date: datetime) -> str:
        """根据日期返回对应的表名"""
        year = date.year
        return f'user_reading_notes_{year}'

    @staticmethod
    def get_current_table() -> str:
        """获取当前年份的表名"""
        return NotesTableRouter.get_table_name(datetime.now())

    @staticmethod
    async def create_note(note_data: dict):
        """创建笔记（自动路由到对应表）"""
        table_name = NotesTableRouter.get_current_table()

        await mysql.execute(f"""
            INSERT INTO {table_name}
            (user_id, book_id, note_type, note_content, created_at)
            VALUES (%s, %s, %s, %s, NOW())
        """, (
            note_data['user_id'],
            note_data['book_id'],
            note_data['note_type'],
            note_data['note_content']
        ))

    @staticmethod
    async def get_user_notes(user_id: int, book_id: int,
                             start_date: datetime, end_date: datetime):
        """查询用户笔记（可能跨多个表）"""
        # 计算需要查询的表
        years = range(start_date.year, end_date.year + 1)
        tables = [f'user_reading_notes_{year}' for year in years]

        # 构建UNION查询
        union_queries = []
        for table in tables:
            union_queries.append(f"""
                SELECT * FROM {table}
                WHERE user_id = %s AND book_id = %s
                AND created_at BETWEEN %s AND %s
            """)

        sql = " UNION ALL ".join(union_queries) + " ORDER BY created_at DESC"

        # 参数重复（每个子查询都需要）
        params = (user_id, book_id, start_date, end_date) * len(tables)

        return await mysql.fetchall(sql, params)
```

#### 自动建表脚本

```python
async def auto_create_table_for_new_year():
    """定时任务：每年1月1日自动创建新表"""
    year = datetime.now().year
    table_name = f'user_reading_notes_{year}'

    # 检查表是否已存在
    exists = await mysql.fetchone(f"""
        SELECT COUNT(*) as cnt
        FROM information_schema.tables
        WHERE table_schema = DATABASE()
        AND table_name = '{table_name}'
    """)

    if exists['cnt'] == 0:
        # 创建新表（从模板复制结构）
        await mysql.execute(f"""
            CREATE TABLE {table_name} LIKE user_reading_notes_template
        """)

        logger.info(f"Auto created table: {table_name}")
```

---

### 4.3 方案2：按用户ID哈希分表（适用于超大规模）

如果单个年份的笔记表仍然过大（如每年10亿+记录），可以进一步分表：

```python
class NotesShardingRouter:
    """笔记表分片路由器（年份 + 哈希）"""

    # 每年分16个表
    SHARD_COUNT = 16

    @staticmethod
    def get_table_name(user_id: int, date: datetime) -> str:
        """根据用户ID和日期计算表名"""
        year = date.year
        shard = user_id % NotesShardingRouter.SHARD_COUNT
        return f'user_reading_notes_{year}_{shard:02d}'

    # 示例表名：
    # user_reading_notes_2025_00
    # user_reading_notes_2025_01
    # ...
    # user_reading_notes_2025_15
```

---

## 5. 数据库读写分离架构

### 5.1 主从架构设计

```
                ┌──────────────┐
                │  Application │
                └───────┬──────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ↓               ↓               ↓
┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│ Write Proxy  │ │  Read Proxy  │ │  Read Proxy  │
│   (Master)   │ │  (Slave 1)   │ │  (Slave 2)   │
└──────┬───────┘ └──────┬───────┘ └──────┬───────┘
       │                │                │
       ↓                ↓                ↓
┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│  MySQL       │ │  MySQL       │ │  MySQL       │
│  Master      │ │  Slave 1     │ │  Slave 2     │
│              │→│              │→│              │
│  (写操作)    │ │  (读操作)    │ │  (读操作)    │
└──────────────┘ └──────────────┘ └──────────────┘
       ↓                                ↓
   Binlog同步                      负载均衡
```

### 5.2 读写分离中间件配置

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# 主库（写）
MASTER_DB_URL = "mysql+pymysql://user:pass@master.db.com:3306/library_app"
master_engine = create_engine(MASTER_DB_URL, pool_size=20, max_overflow=40)

# 从库（读）
SLAVE_DB_URLS = [
    "mysql+pymysql://user:pass@slave1.db.com:3306/library_app",
    "mysql+pymysql://user:pass@slave2.db.com:3306/library_app"
]
slave_engines = [
    create_engine(url, pool_size=50, max_overflow=100)
    for url in SLAVE_DB_URLS
]

class DatabaseRouter:
    """数据库路由器"""

    def __init__(self):
        self.master = master_engine
        self.slaves = slave_engines
        self.slave_index = 0

    def get_write_session(self):
        """获取写会话（主库）"""
        Session = sessionmaker(bind=self.master)
        return Session()

    def get_read_session(self):
        """获取读会话（从库，轮询负载均衡）"""
        engine = self.slaves[self.slave_index]
        self.slave_index = (self.slave_index + 1) % len(self.slaves)

        Session = sessionmaker(bind=engine)
        return Session()
```

---

## 6. 总结与建议

### 6.1 立即实施（P0优先级）

| 优化项 | 预期收益 | 实施难度 | 实施周期 |
|-------|---------|---------|---------|
| 阅读进度Redis缓存 | 写入性能提升10倍+ | 中 | 2-3周 |
| 统计数据预计算 | 查询响应时间从秒级降到毫秒级 | 中 | 2周 |
| 笔记表按年份分表 | 解决未来数据膨胀问题 | 低 | 1周 |

### 6.2 近期规划（P1优先级）

| 优化项 | 预期收益 | 实施难度 | 实施周期 |
|-------|---------|---------|---------|
| 数据库读写分离 | 读取性能提升3倍+ | 高 | 4周 |
| 消息队列解耦 | 系统可扩展性提升 | 中 | 3周 |
| 热点数据监控 | 提前发现性能瓶颈 | 低 | 1周 |

### 6.3 长期演进（P2优先级）

| 优化项 | 预期收益 | 实施难度 | 实施周期 |
|-------|---------|---------|---------|
| 用户ID哈希分表 | 支持10亿+用户规模 | 高 | 6周 |
| 分布式数据库（TiDB） | 弹性扩容能力 | 很高 | 8-12周 |
| OLAP数据仓库 | 支持复杂数据分析 | 高 | 8周 |

---

**文档版本**: 1.0
**最后更新**: 2025-10-14
**下一步**: 查看《02-数据安全深度分析文档》
